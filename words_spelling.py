# -*- coding: utf-8 -*-
"""words_spelling.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1vcLGWmS4B9PVGbHUBtYX3tmKQqmsSUfl
"""

import re
from collections import Counter

"""- We create a function to extract all words in the document:"""

def words(text):
  """
    Tokenize a text into words.

    Args:
    text (str): The input text to be tokenized.

    Returns:
    list of str: A list of words extracted from the input text.

    This function takes a string of text as input and tokenizes it into words. 
    It uses regular expressions to find all sequences of word characters (letters, digits, and underscores)
    in the text and returns them as a list of words in lowercase.
    """
    # Use regular expression to find all word sequences in the text and convert to lowercase
  return re.findall(r'\w+',text.lower())

"""- We load the Norvig's Dataset"""
# Define the filename of the text document to be processed
filename = './data/big.txt'
# Read the contents of the text document, tokenize it into words, and count word frequencies
words_freq = Counter(words(open(filename).read()))
# The Counter object 'words_freq' now contains the frequency of each word in the document.

# Define a function to calculate the probability of a word
def proba(word, N=sum(words_freq.values())):
    """
    Calculate the probability of a word in the document.

    Args:
    word (str): The word for which to calculate the probability.
    N (int, optional): The total number of words in the document. Defaults to the sum of word frequencies.

    Returns:
    float: The probability of the word in the document.

    This function calculates the probability of a word occurring in the document based on its frequency.
    The probability is calculated as the word's frequency divided by the total number of words in the document.
    """
    p = words_freq[word] / N
    return p




"""- We generate all possible corrections of a given word:"""

def first_edit(word):
    """
    Generate a set of all possible edits (1 edit distance) for a given word.

    Args:
    word (str): The input word for which edits are generated.

    Returns:
    set of str: A set containing all possible edits of the input word within 1 edit distance.

    This function generates a set of all possible edits of the input word within 1 edit distance.
    The possible edits include deletions, transpositions, replacements, and insertions of characters.
    """
    # Define the alphabet (all lowercase letters)
    alphabet   = 'abcdefghijklmnopqrstuvwxyz'
    # Create a list of tuples where word is split into two parts at each possible position
    split      = [(word[:i], word[i:])    for i in range(len(word) + 1)]
    # Generate possible edits for each type (delete, transpose, replace, insert)
    delete     = [L + R[1:]               for L, R in split if R] #Deletion
    transpose  = [L + R[1] + R[0] + R[2:] for L, R in split if len(R)>1] #Transpose
    replace    = [L + c + R[1:]           for L, R in split if R for c in alphabet] #Replace
    insert     = [L + c + R               for L, R in split for c in alphabet] #Insertion
    # Combine all possible edits into a set and return
    return set(delete + transpose + replace + insert)

"""- We again generate all possible corrections with a second edit:"""

def second_edit(word):
    """
    Generate a generator of all possible edits (2 edit distances) for a given word.

    Args:
    word (str): The input word for which edits are generated.

    Returns:
    generator of str: A generator that yields all possible edits of the input word within 2 edit distances.

    This function generates all possible edits of the input word within 2 edit distances.
    It utilizes the `first_edit` function to generate first-level edits and then applies `first_edit` again to generate second-level edits.
    """
    # Use a generator expression to generate second-level edits
    return (e2 for e1 in first_edit(word) for e2 in first_edit(e1))

"""- To filter the suggested corrections, we check if each one of them is present within the corpus:"""

def is_in_corpus(words):
  """
    Filter a list of words to include only those that are present in the corpus.

    Args:
    words (list): A list of words to filter.

    Returns:
    set: A set of words that are found in the corpus.

    This function takes a list of words and checks each word against the words in the corpus (dictionary).
    It returns a set containing only the words that are present in the corpus.
    """
    # Use a set comprehension to filter words that exist in the corpus
  return set(w for w in words if w in words_freq)

"""- We get the candidate words:"""

def candidates(word):
    """
    Generate a list of candidate words for correction.

    Args:
    word (str): The word to generate candidate corrections for.

    Returns:
    set: A set of candidate words for correction.

    This function takes a word and generates a set of candidate words for correction.
    It considers the original word, its first-edit, second-edit, and whether it exists in the corpus.
    """
    # Generate a set of candidate words for correction using various methods
    return (
        is_in_corpus([word]) or                  # Original word exists in the corpus
        is_in_corpus(first_edit(word)) or       # Words from first edit in corpus
        is_in_corpus(second_edit(word)) or      # Words from second edit in corpus
        [word]                                  # Use the original word as a fallback
    )


"""- Finally, we get the best correction:"""

def best_corrections(word, n=5):
    """
    Find the best corrected words for a given word.

    Args:
    word (str): The word to find corrections for.
    n (int, optional): The number of best corrections to return. Default is 5.

    Returns:
    list: A list of the top 'n' corrected words for the given input word.

    This function takes a word and returns a list of the top 'n' corrected words based on their probabilities.
    It uses the `candidates` function to generate candidate words and the `proba` function to calculate their probabilities.
    The list is sorted in descending order of probabilities.
    """
    # Generate a list of candidate words for correction
    candidate_list = list(candidates(word))
    
    # Sort candidate words based on their probabilities and return the top 'n' corrections
    return sorted(candidate_list, key=proba, reverse=True)[:n]


"""- We test:"""

input_word = "mov"
corrected_word = best_corrections(input_word,2)
print(f"Input: {input_word}, Corrected: {corrected_word}")

